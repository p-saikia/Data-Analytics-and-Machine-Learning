library(tm)
library(SnowballC)
library(wordcloud)
library(e1071)
library(caret)
rm (list=ls())
cat("\014") 
data <- read.csv('environmental_websites.csv', header=TRUE, stringsAsFactors = FALSE)
str(data)
summary(data)
wordcloud(data$content, min.freq = 500, random.order = FALSE)
scoping <- subset(data$content, data$scoping == "TRUE")
wordcloud(scoping, min.freq = 400, random.order = FALSE)
no_scoping <- subset(data$content, data$scoping == "FALSE")
wordcloud(no_scoping, min.freq = 400, random.order = FALSE)
screening <- subset(data$content, data$screening == "TRUE")
wordcloud(screening, min.freq = 400, random.order = FALSE)
no_screening <- subset(data$content, data$screening == "FALSE")
wordcloud(no_screening, min.freq = 400, random.order = FALSE)
alternative <- subset(data$content, data$alternative == "TRUE")
wordcloud(alternative, min.freq = 400, random.order = FALSE)
no_alternative <- subset(data$content, data$alternative == "FALSE")
wordcloud(no_alternative, min.freq = 400, random.order = FALSE)
document <- subset(data$content, data$document == "TRUE")
wordcloud(document, min.freq = 400, random.order = FALSE)
no_document <- subset(data$content, data$document == "FALSE")
wordcloud(no_document, min.freq = 400, random.order = FALSE)
mitigation <- subset(data$content, data$mitigation == "TRUE")
wordcloud(mitigation, min.freq = 400, random.order = FALSE)
no_mitigation <- subset(data$content, data$mitigation == "FALSE")
wordcloud(no_mitigation, min.freq = 250, random.order = FALSE)
stakeholder <- subset(data$content, data$stakeholder == "TRUE")
wordcloud(stakeholder, min.freq = 400, random.order = FALSE)
no_stakeholder <- subset(data$content, data$stakeholder == "FALSE")
wordcloud(no_stakeholder, min.freq = 400, random.order = FALSE)
nofunctional <- subset(data$content, data$nofunctional == "TRUE")
wordcloud(nofunctional, min.freq = 150, random.order = FALSE)
no_nofunctional <- subset(data$content, data$nofunctional == "FALSE")
wordcloud(no_nofunctional, min.freq = 400, random.order = FALSE)
data$scoping <- factor(data$scoping)
data$screening <- factor(data$screening)
data$alternative <- factor(data$alternative)
data$document <- factor(data$document)
data$mitigation <- factor(data$mitigation)
data$stakeholder <- factor(data$stakeholder)
data$nofunctional <- factor(data$nofunctional)
env_corpus <- VCorpus(VectorSource(data$content))
env_corpus_clean <- tm_map(env_corpus, content_transformer(tolower))
env_corpus_clean <- tm_map(env_corpus_clean, removeNumbers)
env_corpus_clean <- tm_map(env_corpus_clean, removeWords, stopwords())
env_corpus_clean <- tm_map(env_corpus_clean, removePunctuation)
env_corpus_clean <- tm_map(env_corpus_clean, stemDocument)
env_corpus_clean <- tm_map(env_corpus_clean, stripWhitespace)
env_dtm <- DocumentTermMatrix(env_corpus_clean)
train_sample <- createDataPartition(y=data$nofunctional, p=0.75, list = FALSE)
env_dtm_train <- env_dtm[train_sample,]
env_dtm_test <- env_dtm[-train_sample,]
env_freq_words <- findFreqTerms(env_dtm_train, 5)
env_tm_freq_train <- env_dtm_train[, env_freq_words]
env_tm_freq_test <- env_dtm_test[, env_freq_words]
convert_counts <- function(x) {
  x <- ifelse(x > 0, "Yes", "No")
}
env_train <- apply(env_tm_freq_train, MARGIN=2, convert_counts)
env_test <- apply(env_tm_freq_test, MARGIN=2, convert_counts)
env_train_nofunctional_labels <- data[train_sample,]$nofunctional
env_test_nofunctional_labels <- data[-train_sample,]$nofunctional
env_nofunctional_classifier <- naiveBayes(env_train, env_train_nofunctional_labels, laplace = 1)
env_nofunctional_test_pred <- predict(env_nofunctional_classifier, env_test)
confusionMatrix(env_nofunctional_test_pred, env_test_nofunctional_labels, positive = "TRUE")
